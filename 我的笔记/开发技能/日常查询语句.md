# 查询pg中的物理表结构

select
	a.attnum,
	a.attname as field,
	t.typname as type,
	a.attlen as length,
	a.atttypmod as lengthvar,
	a.attnotnull as notnull,
	b.description as comment
from
	pg_class c,
	pg_attribute a
left join pg_description b
    	on
	a.attrelid = b.objoid
	and a.attnum = b.objsubid,
	pg_type t
where
	c.relname = 'dw_lget_company_simple_cancel'
	and a.attnum > 0
and a.attrelid = c.oid  and a.atttypid = t.oid
order by
a.attnum;

# hive操作

## 1.创建表结构

DROP TABLE IF EXISTS tmp_ods_dwd_dw_lget_company_info_df1;
CREATE TABLE tmp_ods_dwd_dw_lget_company_info_df1 (
    `id` STRING COMMENT '记录id',
    `is_delete` STRING COMMENT '是否逻辑删除',
    `create_time` TIMESTAMP COMMENT '创建时间',
    `update_time` TIMESTAMP COMMENT '更新时间',
    `company_code` STRING COMMENT '公司代码_核心字段:1_准确率期望值:100_完整度期望值:100_安全级别:L1',
    `company_name` STRING COMMENT '公司名称_核心字段:1_准确率期望值:100_完整度期望值:100_安全级别:L1',
    `company_name_en` STRING COMMENT '公司英文名称_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `company_sname` STRING COMMENT '公司简称_核心字段:1_准确率期望值:100_完整度期望值:77_安全级别:L1',
    `company_sname_en` STRING COMMENT '公司英文简称_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `reg_code` STRING COMMENT '注册号_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `org_code` STRING COMMENT '组织机构代码_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `uni_code` STRING COMMENT '统一社会信用代码_核心字段:1_准确率期望值:100_完整度期望值:99_安全级别:L1',
    `tax_code` STRING COMMENT '纳税人识别号_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `found_date` STRING COMMENT '成立日期_核心字段:1_准确率期望值:100_完整度期望值:99_安全级别:L1',
    `reg_address` STRING COMMENT '注册地址(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `office_address` STRING COMMENT '办公地址_核心字段:1_准确率期望值:100_完整度期望值:31_安全级别:L1',
    `legal_rep` STRING COMMENT '法定代表人_核心字段:1_准确率期望值:100_完整度期望值:99_安全级别:L1',
    `business_state` STRING COMMENT '经营状态_核心字段:1_准确率期望值:100_完整度期望值:100_安全级别:L1',
    `is_normal` STRING COMMENT '是否正常经营_核心字段:_准确率期望值:_完整度期望值:_安全级别:L2',
    `register_org` STRING COMMENT '登记机关(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `business_scope` STRING COMMENT '经营范围_核心字段:1_准确率期望值:99_完整度期望值:99_安全级别:L1',
    `company_type` STRING COMMENT '公司类型_核心字段:1_准确率期望值:100_完整度期望值:99_安全级别:L1',
    `company_nature` STRING COMMENT '公司性质_核心字段:1_准确率期望值:100_完整度期望值:98_安全级别:L1',
    `company_nature_code` STRING COMMENT '公司性质编码_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `issuance_date` STRING COMMENT '发照日期_核心字段:1_准确率期望值:100_完整度期望值:99_安全级别:L1',
    `end_date` STRING COMMENT '营业期限_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `industry_nea` STRING COMMENT '国民经济行业分类_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `industry_nea_code` STRING COMMENT '国民经济行业代码_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `company_headcount` STRING COMMENT '公司人数(废弃，新表:dw_lget_company_headcount)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `company_size` STRING COMMENT '公司规模(废弃，新表:dw_lget_company_headcount)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `country` STRING COMMENT '国家(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `province` STRING COMMENT '省份(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `city` STRING COMMENT '城市(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `area` STRING COMMENT '区县(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `province_code` STRING COMMENT '省份代码(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `city_code` STRING COMMENT '城市代码(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `area_code` STRING COMMENT '区县代码(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `lng_lat` STRING COMMENT '经纬度(废弃，新表:dw_lget_company_address)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `iconurl` STRING COMMENT '公司logo_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `iconurl_local` STRING COMMENT '本地logo_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `homeurl` STRING COMMENT '公司官网_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `phone` STRING COMMENT '联系电话_核心字段:1_准确率期望值:100_完整度期望值:76_安全级别:L1',
    `email` STRING COMMENT '电子邮件_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `brand_name` STRING COMMENT '品牌名称_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `describer_gen` STRING COMMENT '公司简介(程序生成)_核心字段:1_准确率期望值:100_完整度期望值:100_安全级别:L1',
    `describer` STRING COMMENT '公司简介(公司官网)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `main_business` STRING COMMENT '主营业务_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `regcapital_amt` DOUBLE COMMENT '注册资本(万元)_核心字段:1_准确率期望值:100_完整度期望值:90_安全级别:L1',
    `currency` STRING COMMENT '注册资本(币种)_核心字段:1_准确率期望值:100_完整度期望值:92_安全级别:L1',
    `regcapital_amt_cal` DOUBLE COMMENT '注册资本(计算值(人民币元))_核心字段:1_准确率期望值:100_完整度期望值:92_安全级别:L1',
    `is_medical` STRING COMMENT '是否医健_核心字段:1_准确率期望值:100_完整度期望值:100_安全级别:L2',
    `org_encode` STRING COMMENT '机构编码_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `keyno_code` STRING COMMENT '企查查代码_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `approval_date` STRING COMMENT '核准日期_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `info_source` STRING COMMENT '信息来源_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `paied_amt` STRING COMMENT '实缴资本_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `tax_qualified` STRING COMMENT '纳税人资质_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `emp_exp_code` STRING COMMENT '进出口企业代码(2023-09-11废弃)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `can_date` STRING COMMENT '注销日期_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `rev_date` STRING COMMENT '吊销日期_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `supplier_id` STRING COMMENT '供应商id_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `company_size_std` STRING COMMENT '公司规模标准(废弃，新表:dw_lget_company_headcount)_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1',
    `name_before` STRING COMMENT '曾用名_核心字段:_准确率期望值:_完整度期望值:_安全级别:L1'
) COMMENT '公司基础信息测试表1'
PARTITIONED BY (`ds` STRING) STORED AS PARQUET;

## 2.插入数据

INSERT OVERWRITE TABLE tmp_ods_dwd_dw_lget_company_info_df1 PARTITION(ds = '${bdp.system.bizdate}')
SELECT
    id,
    is_delete,
    create_time,
    update_time,
    company_code,
    company_name,
    company_name_en,
    company_sname,
    company_sname_en,
    reg_code,
    org_code,
    uni_code,
    tax_code,
    found_date,
    reg_address,
    office_address,
    legal_rep,
    business_state,
    is_normal,
    register_org,
    business_scope,
    company_type,
    company_nature,
    company_nature_code,
    issuance_date,
    end_date,
    industry_nea,
    industry_nea_code,
    company_headcount,
    company_size,
    country,
    province,
    city,
    area,
    province_code,
    city_code,
    area_code,
    lng_lat,
    iconurl,
    iconurl_local,
    homeurl,
    phone,
    email,
    brand_name,
    describer_gen,
    describer,
    main_business,
    regcapital_amt,
    currency,
    regcapital_amt_cal,
    is_medical,
    org_encode,
    keyno_code,
    approval_date,
    info_source,
    paied_amt,
    tax_qualified,
    emp_exp_code,
    can_date,
    rev_date,
    supplier_id,
    company_size_std,
    name_before
FROM
    ods_dwd_dw_lget_company_info_df
WHERE
    ds = '20240808';

## 3.分区操作

1.修改分区

ALTER TABLE tb_student PARTITION(cls_p='CS') RENAME TO PARTITION(cls_p='CS2');
2.添加分区：
-- 添加一个新的分区
-- 语法: alter table 表名 add partition (year = '2025', month = '01', day = '08');
alter table score_part_01 add partition (year = '2025', month = '01', day = '08');
-- 可以一次添加多个分区
-- 语法: alter table 表名 add partition() partition(); ==> 不需要,分隔
alter table score_part_01 add partition (year = '2026', month = '01', day = '08')partition (year = '2027', month = '01', day = '08');
3.删除分区：
-- 创建一个分区表
create table tb_student(
    id int,
    name string,
    gender string,
    age int,
    cls string
)partitioned by (cls_p string)
row format delimited fields terminated by ',';

-- show查看分区表的分区信息
show partitions tb_student;

-- 修改分区名
-- 语法: alter table 表名 partition(xxx='老名') rename to partition (xxx='新名');
ALTER TABLE tb_student PARTITION(cls_p='CS') RENAME TO PARTITION(cls_p='CS2');
alter table score_part_01  partition(year = '2026', month = '01', day = '08') rename to partition (year = '2021', month = '01', day = '08');

-- 删除分区
-- 语法: alter table 表名 drop partition(分区字段);
alter table tb_student drop partition(cls_p='CS')

4.动态分区：
-- 创建动态分区表   ，同理分区字段不是表内的字段
drop table user_partition;
create table user_partition(
    id int,
    name string
)partitioned by (dt int)
row format delimited fields terminated by ',';
-- 开启动态分区功能 （添加动态分区之前要先开启动态分区的设置）
SET hive.exec.dynamic.partition=true;
SET hive.exec.dynamic.partition.mode=nonstrict;
--  动态加载数
-- 语法: insert into table 分区表 partition(分区字段) select ...
insert into table user_partition partition(dt) select id,name,dt from test03.test_user;

--查询hive得版本
SELECT version();	--3.1.3 r848294e6edf57a52cee71e52023d255368100409

## 4.优化

桶表的使用

（1）开启分桶功能：set hive.enforce.bucketing=true;
此开关打开之后，会自动根据bucket个数自动分配Reduce task的个数，Reduce个数与bucket个数一致。(此外，Reduce的个数还可以通过mapred.reduce.tasks进行设置，但是这方法不推荐在Hive分桶中使用)。
也可以直接设置reduce的个数
set mapreduce.job.reduces=3;
（2）建表
桶表的数据加载，不能通过hdfs dfs -put文件或者load data，只能通过insert overwrite的方式将普通表的数据通过查询的方式加载到桶表当中去。

创建分桶表（注意分桶字段只能是建表中已有的字段，而分区表的字段必须是建表中没有的字段）

create table bucket(id int, name string, age int) clustered by (age) into 4 buckets row format delimited fields terminated by '\t';
分桶排序表：clustered by (分桶字段) sorted by (排序字段 desc|asc) into n buckets;

 该SQL查询从dw_etl的syn_analysis_stg_task和syn_stg_map表中获取状态为1的任务，结合hive的TBLS、PARTITIONS和PARTITION_PARAMS表，统计表的numRows参数值，并按特定格式提取分区日期。结果将用于更新数据库和表的相关信息。

分桶表应用场景：
1.需要对数据进行频繁的关联查询，或者取样（对未上线的任务，跑部分数据用来验证代码是否正确）的场景
2.对于需要经常进行频繁的聚合操作，如果按照聚合的键进行分桶，可以大大提高查询效率，因为每个节点可以独立完成桶内的聚合操作
3.某个分区的数据量过大，导致查询和处理效率受到影响
优点：
1.对于涉及桶列的join和过滤操作，分桶可以大大加快查询速度（基于桶键的join操作可以在map阶段执行，避免了shuffle和reduce阶段的开销）
2.当查询的谓词(like,in,exists,between)包含分桶键时，可以快速的定位到分桶，极大减少数据扫描范围，提升查询效率

select
a.db_name,a.table_name,a.table_comment,a.source_db,a.source_table,sum(d.param_value) as param_value, SUBSTR(c.part_name,4,10) as part_date,
'dmp' as create_by,'dmp' as update_by
from
(
select
stg_task.db_name,stg_task.table_name,stg_task.table_comment,stg_map.source_db,stg_map.source_table
from
(select db_name,table_name,table_comment from dw_etl.syn_analysis_stg_task where task_status=1) stg_task
inner join
(select target_db,target_table,source_db,source_table from dw_etl.syn_stg_map group by target_db,target_table,source_db,source_table) stg_map
on stg_task.db_name=stg_map.target_db and stg_task.table_name=stg_map.target_table
) a
inner join
(select tbl_id,tbl_name from hive.TBLS) b
on b.tbl_name=a.table_name
inner join
(select tbl_id,part_id,part_name from hive.PARTITIONS) c
on c.tbl_id=b.tbl_id
inner join
(select part_id,param_key,param_value from hive.PARTITION_PARAMS where PARAM_KEY='numRows') d
on d.part_id=c.part_id
group by
a.db_name,a.table_name,a.table_comment,a.source_db,a.source_table, SUBSTR(c.part_name,4,10)
on duplicate key update db_name=values(db_name),table_name=values(table_name),table_comment=values(table_comment),
source_db=values(source_db),source_table=values(source_table),param_value=values(param_value),part_date=values(part_date),
create_by=values(create_by),update_by=values(update_by);

map join优化
使用场景map join通常用于小表与大表之间的连接操作
原理：
1.将小表加载到内存中
2.Mapper在读取大表数据时，会按照Join键对大表的每条记录进行哈希计算
3.Mapper将大表数据的哈希计算结果与内存中加载的小表哈希表进行匹配。
如果匹配成功，则直接将两个表的记录进行连接并输出

数据存储格式和压缩格式：
	在数据加载过程中，选择合适的数据存储格式（对于结构化数据，可以选择Parquet或ORC等列式存储格式；对于非结构化数据，可以选择TextFile或SequenceFile等格式），可以提高查询性能和减少存储空间

-- 添加一个新的分区 as json
-- 语法: alter table 表名 add partition (year = '2025', month = '01', day = '08'); as json
alter table score_part_01 add partition (year = '2025', month = '01', day = '08'); as json
-- 可以一次添加多个分区 as json
-- 语法: alter table 表名 add partition() partition(); ==> 不需要,分隔 as json
alter table score_part_01 add partition (year = '2026', month = '01', day = '08')partition (year = '2027', month = '01', day = '08');

## 5.特殊函数操作

-- 蒙板函数 使用特定字符,将字符进行遮盖, 大写字母用X代替,小写字母用x代替,数字用n代替,其他字符不遮盖
select mask('CHUANzhi123小明%%');
-- 自定义遮盖字符
select mask('CHUANzhi123小明%%', '大', '小', '数');

select mask_first_n('CHUANzhi123', 3); -- 遮盖前n个字符
select mask_last_n('CHUANzhi123', 3); --遮盖后n个字符
select mask_show_first_n('CHUANzhi123', 3); -- 显示前
-- n个字符
select mask_show_last_n('CHUANzhi123', 3); --显示 后n个字符

-- 蒙板函数用的不多, 因为这个函数主要是用于数据脱敏的.
-- 但是大数据开发一般情况下拿到的就是二手数据, 也就是脱敏后的数据.

## 6.分层架构

ods：源数据层，mysql，日志等数据首先被导入到该层。另外dim层层也是属于这个层的，区别在于，ods层的数据要后面要进行处理，而dim层的数据可能会在每一层都被使用，一般在dim层的数据都是不易改变的，哪怕是改变，也是小规模的小数据的改变。
dwd：数据明细层，对空/脏数据数据进行处理。
dwm：数据中间层，只要是根据需求将我们所需的字段连接在一个表中，这个步骤我们一般成为“ 拉宽 ”。
dws： 数据服务层 ，将数据进一步进行筛选，根据维度和指标，获取我们所需求的字段。
ads：数据应用层，也被称为app层、dal层、dm层，叫法繁多。将我们该层的数据交给报表进行分析。一般来说此层的数据都是我们筛选完成我们需求所要的数据。

## 7.批量查询HIVE表分区的行数和数据占用的SQL

SELECT
  table_name
  ,PART_NAME AS partition_name
  ,create_time
  ,num_files
  ,num_rows
  ,raw_data_size
  ,total_size
  -- ,num_files_erasure_coded
  ,transient_last_ddl_time
  ,column_stats_accurate
FROM ( -- 昨天分区
  SELECT PART_NAME,PART_ID,TBL_ID,FROM_UNIXTIME(CREATE_TIME,"%Y-%m-%d %h:%i:%s") AS create_time FROM PARTITIONS
  WHERE PART_NAME=CONCAT("ymd=",DATE_FORMAT(DATE_SUB(CURRENT_DATE(),INTERVAL 1 DAY),"%Y-%m-%d"))
)t1
INNER JOIN ( -- 库名.表名（筛选外部表）
  SELECT CONCAT(DBS.NAME,TBLS.TBL_NAME) AS table_name,TBL_ID
  FROM DBS INNER JOIN TBLS ON DBS.DB_ID=TBLS.DB_ID
  WHERE TBLS.TBL_TYPE="EXTERNAL_TABLE"
  -- OR TBLS.TBL_TYPE="MANAGED_TABLE"
)t0 ON t1.TBL_ID=t0.TBL_ID
INNER JOIN (
  SELECT
    PART_ID
    ,MAX(IF(PARAM_KEY="numFiles",PARAM_VALUE+0,NULL)) AS num_files
    ,MAX(IF(PARAM_KEY="numRows",PARAM_VALUE+0,NULL)) AS num_rows
    ,MAX(IF(PARAM_KEY="rawDataSize",PARAM_VALUE+0,NULL)) AS raw_data_size
    ,MAX(IF(PARAM_KEY="totalSize",PARAM_VALUE+0,NULL)) AS total_size
    ,MAX(IF(PARAM_KEY="numFilesErasureCoded",PARAM_VALUE+0,NULL)) AS num_files_erasure_coded
    ,MAX(IF(PARAM_KEY="transient_lastDdlTime",FROM_UNIXTIME(PARAM_VALUE,"%Y-%m-%d %h:%i:%s"),NULL)) AS transient_last_ddl_time
    ,MAX(IF(PARAM_KEY="COLUMN_STATS_ACCURATE",PARAM_VALUE,NULL)) AS column_stats_accurate
  FROM PARTITION_PARAMS
  GROUP BY PART_ID
  -- HAVING column_stats_accurate IS NOT NULL
)t2 ON t1.PART_ID=t2.PART_ID

## 8. 爆炸函数-explode

（一行转多行，对array数组或者map键值处理）

select
	arr,
	b.id
from(
	select array('1','2','3','4') as arr
) a lateral view explode(arr) b as id

select
	m,
	b.c1,
	b.c2
from(
	select map('id',10086,'name','Tom','age',18) as m
) a lateral view explode(m) b as c1,c2

--质量看板库
-- 查询所有维护表单
select * from dw_lget_date_info where update_freq  <> '暂停维护' or is_delete = 0

-- 查询dwd表与stg表的映射关系
select * from dw_table_time_info where table_en_name = 'dw_lget_company_pledge_equity'

中台数据源接入配置：
1.characterEncoding=UTF-8：字符集设置为UTF-8
2.数据源开启了SSL认证：useSSL=false
3.报错hour_of_day : 2->3 jdbc后面加上默认时区：serverTimezone=UTC
mysql：jdbc:mysql://192.168.201.150:3306/epdb?characterEncoding=UTF-8&useUnicode=true&useSSL=false
kafka:192.168.200.81:30091, 192.168.200.82:30092, 192.168.200.83:30093
PostgreSQL:jdbc:postgresql://192.168.201.30:1921/dm?currentSchema=dws
jdbc:postgresql://192.168.201.23:5432/dw?currentSchema=public

企业信息_开庭公告信息从表
企业信息_股权出质信息从表
dwd_企业信息_法院公告信息表
dwd_企业信息_公司基础信息失信信息表

# 奇点云平台操作

1.即席查询表报错
2.即席查询出来的表分区与普通查询的表分区不一致
（列：查询ods_stg_public_dw_lget_company_court_notice_di表只有一个分区，但是即席查询会有两个分区）

INVALIDATE METADATA;

抽取pg数据库配置条件：
每天增量更新：update_time >= DATE_TRUNC('day', CURRENT_DATE) - INTERVAL '1 day' and update_time < DATE_TRUNC('day', CURRENT_DATE) - INTERVAL '0 day'

每周增量更新：update_time >= DATE_TRUNC('day', CURRENT_DATE) - INTERVAL '7 day' and update_time < DATE_TRUNC('day', CURRENT_DATE) - INTERVAL '0 day'

每月增量更新：update_time >= DATE_TRUNC('day', CURRENT_DATE) - INTERVAL '1 month' and update_time < DATE_TRUNC('day', CURRENT_DATE) - INTERVAL '0 day';

抽取mysql数据库配置条件：
每天增量更新：kp_synctime >= date_sub(CURRENT_DATE,INTERVAL 1 day) and kp_synctime < date_sub(CURRENT_DATE,INTERVAL 0 day)

每周增量更新：kp_synctime >= date_sub(CURRENT_DATE,INTERVAL 7 day) and kp_synctime < date_sub(CURRENT_DATE,INTERVAL 0 day)

调度时间设置每月1号：
每月增量更新：kp_synctime >= DATE_SUB(DATE_FORMAT(NOW(), '%Y-%m-01'), INTERVAL 1 MONTH)
and kp_synctime < DATE_SUB(LAST_DAY(CURRENT_DATE), INTERVAL 1 MONTH)

目前中台不支持该操作
修改表名称：
alter table dim_traffic_site_information_di rename to dim_indc_traffic_site_information_di

--  将历史分区的数据删除，只保留最新分区的一份数据
alter table dim_poli_guide_info_yi drop if exists partition(dt > date_format(date_sub(current_date,7),'yyyyMMdd') and dt < '${bdp.system.bizdate}');

增量更新的数据处理：

背景：目前源头数据会进行多次的修改操作，每次更新后会将该条数据进行修改动作，同时更新修改时间，目前我们中台的接入数据的策略是每天按照修改时间去增量接入前一天的新增或修改的数据，如果不全量进行去重处理，那么输出的数据就会有很多的重复数据，这种数据如果流入下游时会对使用方造成数据问题，为了解决该问题，需要做如下处理；

解决方法：

1.首次取全量数据进行处理，处理时需要加is_delete = 0，取状态有效的数据进行去重操作；
2.后续每天将增量数据接入后，先将每天的增量接入的数据进行处理（治理、脏数据处理），
然后与历史全量的数据进行融合后作去重操作，最后将数据插入到前一天的分区中，当作最最新的一份数据，同时清除历史分区的数据，dwd每天只保留一份全量的数据，对外供数；
3.去重规则：根据主键id分组，更新时间倒序排序，获取最新的一条数据，作为保留；

后续中台任务开发需要优化项：

1）需要将任务名称与表之间建立映射关系（最好将源头数据的表单也整理出，方便问题的排查）；
2）不定期更新的表任务目前按照每天增量来跑（为了及时将更新的数据接入中台（无约定时间，不知道什么时候开始更新了数据）），后续资源不足时可酌情调整调度周期；
3）统计开发任务的调度时间，后续可根据资源使用情况与任务的依赖关系将任务的调度开始时间进行优化；
4）

数据中台优化策略：
1）针对数据量的小的表可以用临时表的方式写入数据(with table as ())，如果是数据量较大时需要创建中间表写入数据（待定，需要与中台侧进行确认）；
2）针对数据量较大的表可以

注意事项：
手动提交任务时，需要指定业务时间，需要注意该业务时间指当前的分区时间；

11月份前共完成85张表的治理
风险：
1.中台试用期：前期中台不稳定，开发过程中遇到了一些中台的bug，部分问题已提交给中台侧来解决，还有一部分是操作问题，与中台侧沟通后已解决；
2.流程标准化：由于前期开发的部分表未应用数据标准，在后续需要将该部分表重新进行应用数据标准，目前有15张左右

11月份治理组计划完成194张表的治理
风险：
1.目前中台操作大批量（千万级以上）数据时，执行速度较慢，后续需要扩充资源；
2.由于外协人员投入开发，后续的工作质量需要投入人力进行保障；
3.目前由于人力资源不足，每个人需要处理的表单数量较多，任务进度可能有延迟的风险；

---

一、阶段性进展
九月份共计完成22张表，十月份共计完成65表;

二、十一月计划安排（截至11月20号）
共计194张表，包含八个主题域;

三、风险分析

1. 11月前问题总结
   （1）中台试用期：在初期的中台开发过程中，存在一些稳定性问题，导致出现若干bug。部分问题已向中台团队反馈并获得解决，而另一些操作问题则在与中台团队沟通后得到解决。
   （2）流程标准迭代：由于数据标准化迭代，前期开发的部分表单需要应用迭代后的标准，后续需要对约15张表进行标准化应用。
2. 11月后风险
   （1）中台操作性能：目前在处理大批量（千万级以上）数据时，中台的执行速度较慢（调优过程中会出现内存溢出或堆空间不足），需考虑后续资源的扩充。@潘瑾瑜
   （2）外协人员质量保障：由于外部人员参与开发，后续工作质量的保障需要额外的人力投入（解决方案：目前投入2人分别跟进业务与技术方面问题，业务@吴雯雯、技术@王元元）。

四、总结
（1）表单情况：三代仓共计1127张表，其中实体关联表418张（实体关联合并为一张大表），其余709张表单。
九、十月份共计完成87张表单，十一月份计划完成196张，11月底剩余426张，其中12月需补充125张，2025年剩余301张表单。
（2）供应商接入计划：预计在12月份从供应商接入数据，在数据中台完成治理工作。
（3）建设成果：在九、十月份共计完成87张表单，同时在数据中台建设94个任务，相较于三代仓的约150个任务，优化了56个任务。

---

系统字段顺序：
系统字段	记录id	id
系统字段	是否逻辑删除	is_delete
系统字段	创建时间	create_time
系统字段	更新时间	update_time
系统字段	清洗时间	etltime

1、实体关联表一对多保留附表，一对一是否维度退化至主表
-----一对一维度退化至主表【新增table_souce、table_source_id、company_code字段】
       一对多拆附表；

2、is_history 是否历史数据字段【统一去除】
     is_locked 是否锁定字段【统一去除】，新增人工修正数据表做白名单覆盖；
     is_valid 是否有效字段【保留】；

3、行政编码要统一成12位

4、数据中台治理规则按照数据集进行大类任务分类，细分按照清洗去重、衍生字段赋值、表单合并进行任务

---

select
	id,
	is_delete
from dwd_lget_company_info
where dt = (select max(dt) from dwd_lget_company_info) and is_delete = 1

ods_dw_public_dw_poif_news_di

1.对于周更月更的任务，若依赖于公司基础信息表或其他每日调度的任务，可不用配置依赖任务，放到下一次时间节点大于该任务执行的时间即可；
2.关联删除表时，可将分区条件改为获取最近两天分区的数据：dt >= replace(date_sub(current_date,2),'-','')

select
	RECID
ods_jy_epdb_deleterec_di
where dt >= replace(date_sub(current_date,2),'-','')
group by RECID

--
replace(replace(replace(replace(replace( replace(replace(replace(replace(replace( replace(replace(replace(replace(replace(replace( certificate_no,'，',','),'。','.'),'！','!'),'？','?'),',',':'),'；',';'),'“','"'),'”','"'),'‘',"'"),'’',"'"),'（','('),'）',')'),'【','['),'】',']'),'—','-'),'…','...')

---

# 查询23pg库的表结构信息

SELECT
    c.relname AS "表名",
    a.attname AS "字段名",
    CASE
        WHEN col_description(a.attrelid, a.attnum) LIKE '%核心字段%' THEN
            CASE
                WHEN POSITION('_' IN col_description(a.attrelid, a.attnum)) > 0 THEN
                    SUBSTRING(col_description(a.attrelid, a.attnum) FROM 1 FOR POSITION('_' IN col_description(a.attrelid, a.attnum)) - 1)
                ELSE
                    col_description(a.attrelid, a.attnum)
            END
        ELSE col_description(a.attrelid, a.attnum)
    END AS "注释",
    CASE
        WHEN format_type(a.atttypid, a.atttypmod) = 'character varying' THEN 'varchar'
        WHEN format_type(a.atttypid, a.atttypmod) LIKE 'character varying%' THEN
            REPLACE(format_type(a.atttypid, a.atttypmod), 'character varying', 'varchar')
        WHEN format_type(a.atttypid, a.atttypmod) = 'smallint' THEN 'int2'
        WHEN format_type(a.atttypid, a.atttypmod) = 'timestamp without time zone' THEN 'timestamp'
        ELSE format_type(a.atttypid, a.atttypmod)
    END AS "类型"
FROM
    pg_class AS c
JOIN
    pg_attribute AS a ON a.attrelid = c.oid
WHERE
    a.attnum > 0
    AND c.relname = 'dw_lget_company_freeze_equity'
ORDER BY
    a.attnum;

---

# 查询ld供应商库的mysql表结构信息

SELECT
    TABLE_NAME AS '表名',
    COLUMN_NAME AS '字段名',
    COLUMN_TYPE AS '类型',
    COLUMN_COMMENT AS '注释'
FROM
    information_schema.COLUMNS
WHERE
    TABLE_SCHEMA = 'data_ky_hs_v2'
    AND TABLE_NAME = 'getjudicialaidinfo';

wangyy
HSwangyy123

-- git提交命令
1.git add .
2.git commit -m "c"
3.git pull
4.git push

# git报错解决：

中止合并，回到合并前的状态

git merge --abort

然后重新拉取代码

git pull origin prod

按 Esc 键

输入 :q!

按 Enter

---

# hive、mysql新增数据量统计

select
name_zh ,table_name ,updatedata_count  ,statistical_date,hierarchical_type
from stg_incr_data_count_config
where statistical_date >= '2025-04-07'
and hierarchical_type in ('Hive','Mysql') order by hierarchical_type,statistical_date

# 查找pg库中表英文名，中文名

SELECT
    c.relname AS table_name,
    d.description AS table_comment
FROM
    pg_catalog.pg_class c
LEFT JOIN
    pg_catalog.pg_namespace n ON n.oid = c.relnamespace
LEFT JOIN
    pg_catalog.pg_description d ON d.objoid = c.oid AND d.objsubid = 0
WHERE
    c.relkind = 'r' -- 只选择普通表
    AND n.nspname = 'public'

---

# dolphi 火石中央数仓(dws任务)

工作流名称
dwm_lada_entity_ind_cl_result 刷的是火石产业标签,国民经济标签,战新产业标签  凌晨执行
dws_lget_patent_info_G1230014  专利表  下午执行
process_is_basic_info_sql_action   读取array_config（*dim_lget_array_config） 刷公司省市区等信息  晚上8.30执行
process_org_info_list           将增量变更数据提前组装org_info_list（*dwm_company_org_info_list_json_table）  这个凌晨执行
process_is_org_info_sql_action   读取array_config 刷org_info_list   这个晚上9点执行 因为dws增量更新在下午

代码general_process
process_carrier_list.py   载体全量刷的代码 增量未做 原因已反馈给业务
process_is_person_list.py  人物list全表刷的代码 增量未做 原因已反馈给业务

carrier_list
依赖dwm表: dwm.dwm_lget_park_rel_info is_delete=0
"表范围: dim.dim_lget_array_config is_delete = 0 and is_carrier_list = 1 读取字段: table_name_en,rel_relation,is_org_info_list"
分两种情况：

1. is_org_info_list = 0
   通过配置表rel_relation对应值与dwm_lget_park_rel_info的table_source_id字段做关联，赋值carrier_list
2. is_org_info_list = 1
   将org_info_list字段explode炸开 取company_id字段并去重 然后与dwm_lget_park_rel_info的table_source_id字段做关联
   " 取carrier_list字段, 之后根据carrier_id字段做group by 将carrier_list聚合并去重(因为可能多个company_id打上同一个载体)"

 第一种情况 可以通过中间表的方式 用sql进行增量更新
 第二种情况 只能全量更新 或者在数据库中做辅助字段做对比更新

org_info_list
依赖dwm表: dwm.dwm_lget_company_rel_info is_delete=0
"表范围: dim.dim_lget_array_config is_delete = 0 and is_org_info_list = 1 读取字段: table_name_en,rel_relation,is_org_info_list"

1. 将每日增量拼接json写入 dwm_company_org_info_list_json_table表
2. 每日目标表增量与dwm_company_org_info_list_json_table表 join 写入对应json
3. 将dwm_company_org_info_list_json_table的每日变更数据（update_time>current_date and create_time<current_date） join各个目标表  然后写入

is_person_list
目前就三个表 是否需要全表都做好join放进去 。。

看dwm_lget_person_rel_info生产逻辑 新闻 文献 专利 各自有各自的人物表 都是用mean_table_id 也就是各自的id做匹配

is_basic_info
依赖dwm表: dwm.dwm_lget_company_label_info is_delete=0 (小疑问： 我是否需要监控逻辑删的数据？ 逻辑删之后如果能有修正好的数据立马进来 那没有问题)

1. 每日目标表增量与dwm_lget_company_label_info表 join 写入对应数据
2. dwm_lget_company_label_info 每日变更数据（update_time>current_date and create_time<current_date） oin各个目标表  然后写入

cl_result更新 这个已经调度上线(凌晨开始跑 做了offset) 我看了正常流程没有问题 基本三小时左右跑完

1. 读取dim.dim_lada_entity_ind_cl_result_config表 以source_table字段与tag_exproess做join
   拿到对应表的数据
   "2. 读取对应表数据 通过dim_lada_chain_level_industry_map/dim_stds_industry_classify 刷上对应的1-10级Code和name,"
   并且拿到对应Map的id，映射为cl_result的map_id
   "3. 任务跑完后 再通过dim_lada_chain_level_industry_map/dim_stds_industry_classify（industry_map,industry_classify方伟做了增量更新的）的update_time 拿到对应修改过的数据"
   根据map_id去更新结果表（原因是 这两张配置表数据会随着指标那边变动 可能修改节点名称啥的）
   "4. 更新完成后 以source_table,source_code做group by 将hs_ind_list，strategic_ind_list,nea_ind_list拼好 存入dwm_lada_entity_ind_cl_result_json_table表(每次先truncate然后insert,这张表每次只留最新数据)"
2. 读取array_config update对应json字段

--py-files hdfs://ns1/user/dolphinscheduler320/default/resources/central-data-warehouse/utils.zip

dwd.dw_lget_patent_info  主表增量 join dw_lget_patent_legal_info 全量更新数据

dws表设计 应该要有两个时间 一个是基础数据更新时间  一个是刷标签的更新时间
一个表要打 火石标签  刷省市区  载体
这三个如果单纯的用update时间去控制一定有问题
先跑哪个脚本 后跑哪个脚本 都会导致多刷一部分数据

所以我想记offset 刷省市区的 一个offset 刷载体的一个offset 这样会加快程序执行速度 各自更新互不干扰

update_time时间 保留 用来datax同步数据
基础数据更新时间  用这个来去更新其他火石标签 公司省市区 载体等数据
标签更新时间   好像可以不要？
要的好处 datax同步到es或者其他gp库之后更新数据可以根据这个时间同步 非基础数据字段数据
    但是需要额外增加datax流程

dorsi数据同步 只能根据update_time时间

目前统一刷的流程

1. 火石标签 战新..标签  以目标表全表join dwm_lada_entity_ind_cl_result_json_table 全表更新 不需要时间依赖
2. basic_info   需要依赖基础数据更新时间字段为什么？
   1.5号下午我根据配置表全量刷了一遍省市区字段 但是在没办法记录每个表那一批数据的最后update_time时间 （
   大表一定会出现程序在刷省市区的过程中 进来新数据的情况
   ） 我周一来执行增量就没有办法做
   如果有基础数据更新时间 我就可以根据这个时间 去刷后面未刷的数据 包括之后放假因为各种问题导致代码中间没跑 也可以用这个时间补救
3. org_info_list  增量同理
4. person_list 增量同理
5. carrier_list 增量同理

array_config rel_relation='company_id'的表有28张 -> 三个标签json，is_carrier_list与is_basic_info

dws_lget_ipo_appl is_basic_info=1  rel_relation都为 company_id
如果company_id被逻辑删除

1. 主表增量更新(不管是新的增量还是因为程序把update时间变了 都没有问题) 在关联各自rel表拿到company_id表的时候 写入dws表 --- 没有问题 关联不上就是为空
   "   (1), 假如原来dws表有company_id值 但是后续更新置为null了 "
   解决办法: 根据基础数据更新时间字段 筛选出company_id为null 读取配置表 rel_relation为company_id 且配置字段为1 置为null
   "   (2), 假如company_id值变了  没有关系  配置表后续流程 根据变了的字段去刷就可以了"
2. 副表 _rel表的增量反哺dws表的时候（_rel表数据更新 但是主表未更新这部分数据）
   什么情况会出现？-》 正常_rel表数据更新是要根据主表的增量数据 去更新rel表里数据？ 那一定有定时刷_rel表历史数据的操作
   这部分数据 需要join主表全量  拿到更新id 更新company_id到dws表

间接有关系的 org_info_list的有 11张  rel_relation='mean_table_id'
中间表dwm_lget_company_rel_info 记录各个表的实体关联信息(其实就是拿company_id)
这个更新我做了一张dwm_company_org_info_list_json_table 中间表 会把rel_info正常更新或者逻辑删的数据组装成json 之后关联目标表写入

carrier_list 依赖org_info_list任务完成

is_org_info_list = 0 and is_basic_info=1 rel_relation=company_id
原表company_id置为null 按照流程刷就行了 （要求carrier_list表的更新要在dws更新之前 做好任务依赖才行）

is_org_info_list = 0 and is_basic_info=0 rel_relation=mean_table_id
mean_table_id不可能置为null 按照流程刷就行了

is_org_info_list = 1  炸开org_info_list用company_id刷

---

数开:
服务器:
192.168.201.21-25 (新仓spark服务器,dolphi也在上面-找潘瑾瑜)
192.168.202.31-35(doris服务器)
host映射:
192.168.201.21 bigdata01.hs.com bigdata01
192.168.201.22 bigdata02.hs.com bigdata02
192.168.201.23 bigdata03.hs.com bigdata03
192.168.201.24 bigdata04.hs.com bigdata04
192.168.201.25 bigdata05.hs.com bigdata05

Yarn
http://bigdata02.hs.com:8088/cluster/apps/RUNNING
历史服务器:
http://192.168.201.25:18080/

Dolphi:
    http://192.168.201.25:12345/dolphinscheduler/ui/ (问潘要)  (dwd-dws,dwd-dwm这两层， datax从gp-doris 也在上面调度-> 火石中央数仓-服务层同步)
有个问题： dolphi资源中心 上传git代码用dolphi接口的方式行不通, 目前的方式是在201.25服务器上用hadoop的方式上传

git地址:
https://git.aihuoshi.net/algo_analysis_plat/data-engineering/datax-core (dolphi下配置的datax)

https://git.aihuoshi.net/algo_analysis_plat/data-engineering/central-data-warehouse (数开 代码)
我的代码主要在general_process下,主要是一些通用流程的代码(火石标签,战新标签,carrier_list，org_info_list等)

产业智治文档地址-表结构已通过审核(规则文档)
汇总层表结构变动登记清单

---

# mysql库中查询表的数据量

SELECT
    TABLE_NAME,
    TABLE_ROWS
FROM
    INFORMATION_SCHEMA.TABLES
WHERE
    TABLE_SCHEMA = '你的数据库名';
优点：非常快速，不需要扫描实际表数据

可以一次性获取所有表的信息

缺点：对于 InnoDB 表，TABLE_ROWS 是估计值，不是精确计数

统计信息可能不是最新的

sparkpy程序中写入字段值报：ValueError: A string literal cannot contain NUL (0x00) characters.
时，因为有异常值插入，可用：replace(b.enterprisecode, char(0), '')

---

pgsql中匹配两个字段的包含关系，a中包含b，或者b中包含a（如果不考虑相互关系，可以将ON后面的条件删掉一个，只保留一个即可）
SELECT
    a.park_name,
    a.cleaned_name_a,
    b.province AS b_province,
    b.area AS b_area,
    b.city AS b_city,
    b.address AS b_address,
    b.park_code AS b_park_code,
    b.park_name AS b_park_name,
    b.id AS b_id
FROM
(
    SELECT *,
           REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(LOWER(park_name),
           '重庆|市|区|园|[-.,，、（）()]', ''), '\\s+', ''), '产业基地|产业园区|开发区|工业园', ''), '有限公司|公司', '') AS cleaned_name_a
    FROM dw_rsel_industrial_park
    WHERE is_delete = 0 AND province = '重庆市'
) a
JOIN
(
    SELECT *,
           REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(REGEXP_REPLACE(LOWER(park_name),
           '重庆|市|区|园|[-.,，、（）()]', ''), '\\s+', ''), '产业基地|产业园区|开发区|工业园', ''), '有限公司|公司', '') AS cleaned_name_b
    FROM dw_rsel_industrial_park
    WHERE is_delete = 0 AND province = '重庆市'
) b ON (a.cleaned_name_a LIKE '%' || b.cleaned_name_b || '%'
        OR b.cleaned_name_b LIKE '%' || a.cleaned_name_a || '%')
WHERE a.park_name <> b.park_name

---

sparksql处理科学计数法转为正常显示的数据问题
SELECT conratio,

 CASE
    WHEN conratio LIKE '%E%' THEN
      CAST(SPLIT(conratio, 'E')[0] AS DOUBLE) *
      POWER(10, CAST(SPLIT(conratio, 'E')[1] AS DOUBLE))
    ELSE CAST(conratio AS DOUBLE)
  END AS calculated_value

FROM  stg_longdun_getshareholderinfo_incr_dt where id ='f200feca5292843aaa8e199267bbbf95';

---

报错：psycopg2.errors.ProgramLimitExceeded: index row size 2864 exceeds btree version 4 maximum 2704 for index "bk_dw_lget_adm_license_change_dw_lget_abro"
DETAIL:  Index row references tuple (32961,15) in relation "dw_lget_adm_license_change".
HINT:  Values larger than 1/3 of a buffer page cannot be indexed.
Consider a function index of an MD5 hash of the value, or use full text indexing.

解决办法：
修改原表的索引字段
DROP INDEX IF EXISTS bk_dw_lget_adm_license_change_dw_lget_abro;

CREATE INDEX idx_dw_lget_license_change_main ON test.dw_lget_adm_license_change
(company_id, change_item, change_date);

---

1."hive": 用于将数据写入Hive表。这是与Hive集成的一个方式，允许你在Hive中管理数据。

2."parquet": Parquet是一种列式存储格式，非常适合于大数据场景。它提供了高效的数据压缩和编码方案，同时保持了数据的可伸缩性和查询性能。

3."orc": ORC（Optimized Row Columnar）是另一种列式存储格式，它也专注于提高查询性能和压缩效率。ORC格式在Apache Hive中广泛使用。

4."avro": Avro是一种数据序列化系统，支持丰富的数据结构。它非常适合于跨语言的数据交换，并且可以有效地处理复杂的数据类型。

5."json": JSON是一种轻量级的数据交换格式，易于人类阅读和编写，同时也被许多编程语言支持。

6."csv": CSV（逗号分隔值）是一种简单的文本格式，用于存储表格数据。尽管它不如其他一些格式（如Parquet或ORC）高效，但在某些情况下仍然很有用，特别是数据导出到其他系统时。

7."text": 简单的文本文件格式，每行一个记录。这种格式简单但不够高效，通常不推荐用于生产环境的大数据存储。

要使用这些不同的格式，你可以在调用 .write 方法后使用 .format() 方法指定格式，

1.将hdfs上的文件数据映射到hive表
！！注意：行列分隔符，文件格式必须与hdfs中的文件格式一致
CREATE TABLE stg.stg_longdun_getSoftwareCopyrightInfo_incr_dt_full_textfile(
	entid string,
	entname string,
	frj_rjqc string,
	frj_rjjc string,
	frj_bbh string,
	frj_rjflh string,
	frj_hyflh string,
	frj_scfbdate string,
	frj_djdate string,
	frj_djh string,
	frj_zzqr_gj string,
	change_type string,
	id string
)

ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/user/lddata/getSoftwareCopyrightInfo';

---

CREATE TABLE stg.stg_longdun_getWorksCopyrightInfo_incr_dt_full_textfile(
	entid string,
	entname string,
	fzd_djh string,
	fzd_zpmc string,
	fzd_zplb string,
	fzd_czwcdate string,
	fzd_scfbdate string,
	fzd_djdate string,
	fzd_zzname string,
	change_type string,
	id string
)

ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
STORED AS TEXTFILE
LOCATION '/user/lddata/getWorksCopyrightInfo';

2.将表写入到压缩过的hive表中，节省存储空间，参考以下代码；
import sys
import time
import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, col
from pyspark.sql.types import StringType, IntegerType, DateType, TimestampType

def get_stg_incr(spark, dt):
    dw_sql = f"""
    select
        entid,
        entname,
        fzd_djh,
        fzd_zpmc,
        fzd_zplb,
        fzd_czwcdate,
        fzd_scfbdate,
        fzd_djdate,
        fzd_zzname,
        change_type,
        id
    from stg.stg_longdun_getWorksCopyrightInfo_incr_dt_full_textfile
    """

    # 执行SQL并获取结果DataFrame
    dw_df = spark.sql(dw_sql).dropDuplicates(["id"])
    dw_df = dw_df.na.replace("", None)

    # 添加分区列
    dw_df = dw_df.withColumn("dt", lit(dt))

    # 打印schema检查类型
    print("=== DataFrame Schema ===")
    dw_df.printSchema()

    # 首次需解开，创建结果表
    spark.sql("DROP TABLE IF EXISTS stg.stg_longdun_getWorksCopyrightInfo_incr_dt_full")
    spark.sql("""
    CREATE TABLE IF NOT EXISTS stg.stg_longdun_getWorksCopyrightInfo_incr_dt_full (
        entid string,
        entname string,
        fzd_djh string,
        fzd_zpmc string,
        fzd_zplb string,
        fzd_czwcdate string,
        fzd_scfbdate string,
        fzd_djdate string,
        fzd_zzname string,
        change_type string,
        id string
    )
    PARTITIONED BY (dt STRING)
    STORED AS PARQUET
    """)

    # 写入数据
    # overwrite
    dw_df.write.mode("append").insertInto("stg.stg_longdun_getWorksCopyrightInfo_incr_dt_full")

    # df.write.option("dynamicPartitionOverwrite", "true").partitionBy("date").format("parquet").save(
    #     "/path/to/destination")

    print(f"写入完成，数据量：{dw_df.count()}")

if __name__ == '__main__':
    args = sys.argv
    if len(args) < 2:
        date = (datetime.datetime.now() - datetime.timedelta(days=1)).strftime('%Y%m%d')
    else:
        date = args[1]
        try:
            time.strptime(date, '%Y%m%d')
        except Exception as e:
            raise ValueError(f'Invalid date arg: {date}')

    session = ((SparkSession
    .builder
    .appName('stg_longdun_getWorksCopyrightInfo_incr_dt_full')
    .enableHiveSupport()
    .config('hive.exec.dynamic.partition.mode', 'nonstrict')
                .config('hive.exec.dynamic.partition', 'true'))
               .config('spark.sql.sources.partitionOverwriteMode', 'dynamic')
               .config('spark.executor.memory', '20g')
    .config('spark.executor.instances', '16')
    .config('spark.sql.shuffle.partitions', 400)
    .config('spark.executor.cores', '1')
    .config('spark.driver.memory', '16g')
    .getOrCreate())

    get_stg_incr(session, date)

---

azkaban调度任务：

任务名称：data_init_longdun（ld数据接入与数据量统计任务）

new_data_count与new_data_daily作业手动跑时输入参数是：begin_date=前一天日期，end_date=当天日期，格式：(yyyyMMdd)
PG50_data_count、dm_data_count、doris_data_count、hive_data_count手动跑时，传入的参数是：date=前一天日期，格式(yyyyMMdd)

pg死锁查询:
SELECT blocked_locks.pid AS blocked_pid,
       blocked_activity.usename AS blocked_user,
       blocking_locks.pid AS blocking_pid,
       blocking_activity.usename AS blocking_user,
       blocked_activity.query AS blocked_statement,
       blocking_activity.query AS blocking_statement
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks
    ON blocking_locks.locktype = blocked_locks.locktype
    AND blocking_locks.DATABASE IS NOT DISTINCT FROM blocked_locks.DATABASE
    AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation
    AND blocking_locks.page IS NOT DISTINCT FROM blocked_locks.page
    AND blocking_locks.tuple IS NOT DISTINCT FROM blocked_locks.tuple
    AND blocking_locks.virtualxid IS NOT DISTINCT FROM blocked_locks.virtualxid
    AND blocking_locks.transactionid IS NOT DISTINCT FROM blocked_locks.transactionid
    AND blocking_locks.classid IS NOT DISTINCT FROM blocked_locks.classid
    AND blocking_locks.objid IS NOT DISTINCT FROM blocked_locks.objid
    AND blocking_locks.objsubid IS NOT DISTINCT FROM blocked_locks.objsubid
    AND blocking_locks.pid != blocked_locks.pid
JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.GRANTED;

---

# datax任务创建步骤

登录datax平台 ->
任务构建 ->
按照提示填连接信息，点击下一步 ->
在最后一步时需要添加执行器，选择模板即可 ->
然后将json语句复制进去 ->

reader：数据源

writer：结果表

语句如下：
{
  "job": {
    "setting": {
      "speed": {
        "channel": 3
      },
      "errorLimit": {
        "record": 0,
        "percentage": 0.02
      }
    },
    "content": [
      {
        "reader": {
          "name": "postgresqlreader",
          "parameter": {
            "username": "nwpvM3K4NQJentsknENRLA==",
            "password": "seVKG1srzhUPb5JS1PHPb38sxuRddOv83hdqDOVR+i0=",
            "column": [
                `<!-- 实际需要同步的数据源表字段 -->`
              "\"id\"",
              "\"is_delete\"",
              "\"stock_code\"",
              "\"security_shortname\"",
              "\"raport_date\"",
              "\"class_type\"",
              "\"main_business\"",
              "\"main_operating_income\"",
              "\"income_ratio\"",
              "\"main_operating_expense\"",
              "\"expense_ratio\"",
              "\"main_operating_profit\"",
              "\"profit_ratio\"",
              "\"profit_margin_ratio\""
            ],
            "where": "update_time>${lastTime} and update_time<=${currentTime}",
            "splitPk": "",
            "connection": [
              {
                "table": [
                    `<!-- 源表名 -->`
                  "public.dw_lget_bm_financial_structure_data"
                ],
                "jdbcUrl": [
                  "jdbc:postgresql://192.168.201.23:5432/dw"
                ]
              }
            ]
          }
        },
        "writer": {
          "name": "gpdbwriter",
          "parameter": {
            "username": "0BjZFuIDrUOL1661wAz5IA==",
            "password": "SjRz9ma9kPTTVcDmkicAkX8sxuRddOv83hdqDOVR+i0=",
            "column": [
                `<!-- 实际需要同步的目标表字段 -->`
              "id",
              "is_delete",
              "stock_code",
              "security_shortname",
              "raport_date",
              "class_type",
              "main_business",
              "main_operating_income",
              "income_ratio",
              "main_operating_expense",
              "expense_ratio",
              "main_operating_profit",
              "profit_ratio",
              "profit_margin_ratio"
            ],
            "writeMode": "update(id)",
            "connection": [
              {
                "table": [
                    `<!-- 目标表名 -->`
                  "dwd.dw_lget_bm_financial_structure_data"
                ],
                "jdbcUrl": "jdbc:postgresql://192.168.201.30:1921/dm"
              }
            ]
          }
        }
      }
    ]
  }
}

# postgresql库数据备份操作步骤

    详细操作步骤见以下文档：[PG库数据备份操作步骤](./资料/PG库数据备份操作步骤.docx)


# ES命令


## 查看索引映射

GET /test_dm_cydn_cfda_clinical_drug_infos/_mapping

## 查看索引状态

GET /test_dm_cydn_cfda_clinical_drug_infos/_settings

## 查看所有索引

GET /_cat/indices?v

## 查询所有数据并限制条数

GET /dm_cydn_cfda_clinical_drug_infos/_search
{
  "query": {
    "match_all": {}
  },
  "size": 100
}

## 查询数据量

GET /dm_cydn_cfda_clinical_drug_infos/_count

## 查询字段映射

GET /dm_cydn_cfda_clinical_drug_infos/_mapping

## 根据条件查询数据量

GET /dm_map_literature_1/_count
{
  "query": {
    "range": {
      "SYS_dm_update_time": {
        "gte": "2025-09-17T00:00:00",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd'T'HH:mm:ss"
      }
    }
  }
}
