### 一、 核心概念与理论面试题

这类问题用于考察你对ETL基础的理解是否扎实。

1. **请解释什么是ETL？它的主要阶段是什么？**
   * **E（Extract）** ：从各种异构数据源（数据库、API、日志文件、CSV等）抽取数据。
   * **T（Transform）** ：对抽取的数据进行清洗、转换、标准化、聚合、关联等操作，使其符合目标系统的数据模型和质量要求。这是核心价值所在。
   * **L（Load）** ：将转换后的数据加载到目标数据仓库、数据湖或数据库中。
2. **ETL 和 ELT 有什么区别？各自的优缺点和适用场景是什么？**
   * **ETL** ：在加载前进行转换。传统方式，转换在专门的ETL服务器上进行。
   * *优点* ：对目标系统压力小，安全性高（敏感数据在加载前可被脱敏）。
   * *缺点* ：转换能力受ETL服务器性能限制，不够灵活。
   * **ELT** ：先加载到目标系统，再利用目标系统（如云数据仓库Snowflake、BigQuery）的强大计算能力进行转换。
   * *优点* ：充分利用云原生计算能力，处理海量数据更快更灵活。
   * *缺点* ：对目标系统压力大，数据安全性和治理要求更高。
   * *场景* ：现代数据栈更倾向于ELT，特别是在云数据仓库环境中。
3. **什么是数据仓库？它的特点是什么？（星型模型 vs 雪花模型）**
   * **定义** ：面向主题的、集成的、非易失的、随时间变化的数据集合，用于支持管理决策。
   * **星型模型** ：一个大的中心事实表，周围连接多个维度表。 **查询简单，性能高** ，是首选。
   * **雪花模型** ：维度表被规范化成多个表。 **节省存储空间** ，但查询更复杂，需要更多JOIN。
4. **什么是SCD（缓慢变化维）？请举例说明Type 1, Type 2, Type 3。**
   * **核心问题** ：当维度属性发生变化时，在数据仓库中如何记录历史？
   * **Type 1（覆盖）** ：直接更新旧值，不保留历史。适用于纠正错误。
   * **Type 2（增加新行）** ：最常用。添加新行，并标记生效日期/失效日期或使用代理键。 **完整保留历史** 。
   * **Type 3（增加新列）** ：增加新列来记录旧值，只能保留有限的历史（如上一次变化）。
5. **数据清洗通常包括哪些步骤？**
   * 处理空值（填充或剔除）
   * 数据格式标准化（日期、电话号）
   * 数据验证与去重
   *  **数据解析** （从JSON/XML中提取字段）
   * 处理异常值和错误数据

---

### 二、 技术技能与工具面试题

这部分会深入考察你对具体技术和工具的掌握程度。

#### SQL（必考，通常是笔试或在线测试）

1. **核心语法** ：`JOIN`（各种连接的区别）、`WHERE`、`GROUP BY`、`HAVING`、`ORDER BY`。
2. **窗口函数** ：`ROW_NUMBER()`, `RANK()`, `DENSE_RANK()`, `LEAD()`, `LAG()`，以及 `OVER (PARTITION BY ... ORDER BY ...)` 子句。这是面试高频考点。
3. **性能优化** ：

* 如何排查和优化一条慢SQL？
* 索引的作用和原理？什么情况下索引会失效？
* 解释执行计划（`EXPLAIN`）你会看哪些关键指标？

#### ETL/ELT 工具

1. **传统ETL工具** ：Informatica, DataStage, SSIS。如果你熟悉，准备一个你设计包或流程的例子。
2. **开源/现代工具** ：

* **Apache Airflow** ：如何编写一个DAG？什么是Operator、Sensor？如何管理依赖和处理失败？
* **dbt（核心）** ：如何编写模型（model）和测试（test）？什么是物化（View, Table, Incremental）？如何引用宏（Jinja）？如何管理上下游依赖？
* **Apache Spark** ：RDD vs DataFrame vs Dataset？什么是宽依赖和窄依赖？如何调优Spark作业（分区、内存、并行度）？

#### 编程语言

1. **Python** ：

* 如何处理CSV/JSON文件？（`pandas`, `json`库）
* 如何连接数据库并进行操作？（`sqlalchemy`, `pymysql`等）
* `Pandas`的常用操作：`groupby`, `merge`, `apply`，如何处理大数据集（分块）？

1. **Shell Script** ：常用命令（`grep`, `awk`, `sed`, `find`），用于文件处理和任务调度。

#### 数据仓库与大数据平台

1. **云数据仓库** ： **Snowflake** （虚拟仓库、微分区、零拷贝克隆）、 **BigQuery** （无服务器、按查询付费）、 **Redshift** （集群架构、列式存储）。
2. **Hadoop生态** ：HDFS, Hive, HBase的基本原理。虽然热度下降，但大公司可能仍会问。

---

### 三、 场景设计与项目经验面试题

这是面试的核心，考察你解决实际问题的能力。

1. **项目介绍**
   * **准备一个你最熟悉的ETL项目** ，使用 **STAR法则** （情境、任务、行动、结果）来描述。
   * 清晰说明：数据来源、数据量、处理逻辑、用了什么技术栈、最终输出了什么、业务价值是什么。
2. **场景设计题**
   * **“如何设计一个从MySQL到数据仓库的增量数据同步流程？”**
     * 关键点：如何识别增量数据？使用 `last_modified`时间戳、数据库日志（CDC）、还是自增ID？
   * **“有一个日增10GB的日志文件，如何设计ETL流程将其入库并生成日报？”**
     * 关键点：考虑使用Spark进行分布式处理，设计分区策略，调度频率（每日），目标表结构。
   * **“在数据处理过程中，如何保证数据的准确性和一致性？”**
     * 关键点：数据质量检查（非空、唯一性、值域）、数据校验（记录数核对、金额合计核对）、任务监控和告警。
   * **“如果任务在凌晨3点失败了，你会如何排查和修复？”**
     * 关键点：查看日志 -> 定位错误原因（数据问题？资源问题？）-> 修复问题 -> 重新运行任务 -> 检查后续依赖任务 -> 通知相关人员。

---

### 四、 行为面试与软技能

1. **你如何管理你的数据管道/任务的依赖关系？**
   * 考察你对工作流调度工具（如Airflow）的理解和使用经验。
2. **当你和业务方对某个数据指标的定义有分歧时，你会怎么做？**
   * 考察沟通能力和数据治理意识。答案应偏向于：沟通确认、查阅文档、寻求共识、最终形成规范文档。
3. **你是如何学习新技术（如dbt, Snowflake）的？**
   * 考察学习能力和主动性。可以提及官方文档、线上课程、实践项目、技术社区等。
4. **描述一次你处理紧急数据问题的经历。**
   * 使用STAR法则，重点突出你的 **问题排查能力** 、**抗压能力**和 **结果** 。

---

### 五、 面试官可能会问你的问题

* 你对我们公司的数据团队/业务有什么了解？
* 你为什么想离开现在的公司？
* 你的职业发展规划是什么？

---

### 六、 你可以向面试官提问的问题

这体现了你的思考深度和求职诚意。

* 团队目前主要的数据栈（Tech Stack）是什么？接下来有引入新技术的计划吗？
* 我入职后主要负责的数据领域和业务线是什么？
* 团队的数据治理和开发规范是怎样的？（比如代码审查、CI/CD）？
* 您认为这个职位最大的挑战是什么？
* 公司对数据团队的支持和定位是怎样的？

### 最后的建议

* **复习你的简历** ：确保你能清晰地解释简历上的每一个项目和技术点。
* **刷SQL题** ：LeetCode、HackerRank上的中等难度题目足够。
* **保持自信和沟通** ：即使问题很难，也要展示出你的思考过程，面试官看重的是解决问题的思路。
