### 一、 大数据开发岗位核心认知

面试官希望找到的候选人，不仅仅是会写代码，更要理解数据的价值和处理数据的整体架构。

1. **核心职责** ：

* **数据接入** ：从各种数据源（业务数据库、日志、爬虫、IoT设备）高效、稳定地采集数据。
* **数据存储与处理** ：设计和维护大数据存储（HDFS, HBase, Kafka等），编写ETL/ELT任务，对海量数据进行清洗、转换、集成。
* **数据建模** ：根据业务需求，设计数据仓库（如维度建模）、数据湖、数据湖仓一体化的数据模型。
* **数据服务与开发** ：为数据分析师、数据科学家和业务应用提供稳定、可靠的数据接口和数据产品（如报表、数据API）。
* **平台运维与优化** ：保障大数据集群的稳定、高效运行，对任务进行性能调优。

1. **技术栈特点** ：

* **分层架构** ：通常分为数据采集层、存储层、计算层、服务层。
* **组件化** ：大量使用开源生态组件，需要理解每个组件的适用场景和优缺点。
* **编程语言** ： **Scala/Java** （核心组件开发）、 **SQL** （数据处理核心）、 **Python** （脚本、数据探查）、 **Shell** （调度、运维）。

---

### 二、 核心技术栈与技能要求

这是面试考察的重点，你需要分层掌握。

#### 1. 编程语言基础

* **Java/Scala** ：
* **Java** ：集合框架、多线程与并发（JUC包）、JVM内存模型与GC、IO/NIO。
* **Scala** ：函数式编程、特质（Trait）、样例类（Case Class）、集合、隐式转换（了解即可）。
* **SQL** ：这是 **重中之重** ！窗口函数、性能优化（Explain执行计划、索引、分区）、复杂逻辑编写。
* **Python** ：常用库：Pandas, PySpark。主要用于数据清洗、分析和脚本编写。

#### 2. 大数据生态核心框架

* **Hadoop** （基石）：
* **计算引擎** ：
* **Spark** （核心中的核心）：
  * 核心概念：RDD、DAG、宽窄依赖。
  * 执行流程：从代码到Task执行的全过程。
  * **Spark SQL** ：Catalyst优化器、Tungsten执行引擎。
  * **性能调优** ：数据倾斜解决方案（加盐）、内存调优、并行度设置。
* **Flink** （流处理核心）：
  * 核心概念：时间语义（Event Time, Processing Time）、Watermark、状态管理、Checkpoint机制。
  * 与Spark Streaming的区别（微批 vs. 真流式）。
* **数据采集与传输** ：
* **Kafka** ：架构（Broker, Topic, Partition）、高吞吐原理、数据可靠性保证、重复消费与漏消费问题。
* **Flume** / **Sqoop** /  **DataX** ：了解使用场景和基本原理。
* **资源调度与协调** ：
* **Zookeeper** ：分布式一致性原理、应用场景（选举、分布式锁）。
* **OLAP引擎** （加分项）：
* **ClickHouse** 、 **Doris** 、 **Kylin** ：了解其预聚合、MPP架构的特点和适用场景。
* **工作流调度** ：
* **DolphinScheduler** 、 **Azkaban** 、 **Airflow** ：了解核心概念（DAG、任务依赖）和使用方法。

### **三、HDFS：分布式存储基石**

HDFS的设计目标是存储超大规模的数据集，并运行在普通的商用硬件集群上。它的核心思想是： **一次写入，多次读取** 。

#### **1. 架构**

HDFS采用主从架构。

* **NameNode**

  * **角色** ：主节点，只有一个（或在HA中有两个互为备用的），是系统的“大脑”。
  * **职责** ：

  1. **管理文件系统的命名空间** ：存储文件的元数据，如文件名、目录结构、文件属性（创建时间、副本数等）。
  2. **维护文件系统的树状结构**和文件与数据块的映射关系。
  3. **管理数据块的映射信息** ：知道每个文件被切分成哪些数据块，以及这些数据块分别存储在哪些DataNode上。
  4. **协调客户端的读写请求** 。

  * **重要点** ：NameNode**不存储**实际的数据块，只存储元数据。元数据在启动后会加载到内存中，以实现快速访问。
* **DataNode**

  * **角色** ：从节点，有多个，是系统的“肌肉”。
  * **职责** ：

  1. **存储实际的数据块** 。
  2. **执行数据块的读写操作** 。
  3. **定期向NameNode发送心跳** （默认3秒一次）和块报告。心跳表示自己“活着”，块报告告知NameNode自己持有哪些数据块。

  * **重要点** ：如果NameNode长时间收不到某个DataNode的心跳，就认为该节点失效，会将其上的数据块在其他存活的DataNode上重新复制，以达到预设的副本数。
* **Secondary NameNode**

  * **注意** ：它不是NameNode的热备，不能实现高可用。
  * **职责** ：定期合并NameNode的编辑日志到镜像文件中， **减轻NameNode的负担** ，并保存一个NameNode元数据的检查点，以防NameNode完全崩溃时可以恢复部分数据。在HA架构中，它的角色被Standby NameNode取代。

#### **2. 读写流程**

* **写流程** ：

1. **客户端**向NameNode发起写文件请求。
2. NameNode检查文件是否存在以及客户端权限，若通过，则在命名空间中创建文件记录，并返回给客户端一个可写的DataNode列表（通常是多个，以满足副本数要求）。
3. 客户端将文件切分成多个 **数据包** ，并写入一个 **本地磁盘缓存** 。然后，数据包被推送到**管道**中的第一个DataNode。
4. 第一个DataNode接收数据包，将其写入本地磁盘，然后将其转发给管道中的第二个DataNode。
5. 第二个DataNode做同样的事情，转发给第三个DataNode。如此往复，形成一个 **写管道** 。
6. 管道中最后一个DataNode完成写入后，会反向发送一个确认包。最终，第一个DataNode将确认包返回给客户端。
7. 当一个数据块的所有副本都写入成功后，客户端会收到确认，然后开始发送下一个数据包。

* **读流程** ：

1. **客户端**向NameNode请求获取文件的数据块位置列表（包含每个数据块的所有副本位置）。
2. NameNode返回包含数据块及其所有副本DataNode地址的列表。
3. 客户端直接与**最近**的（根据网络拓扑距离）DataNode建立连接，读取数据块。
4. 数据从DataNode流式地传输给客户端。
5. 当一个数据块读取完毕，客户端会关闭与该DataNode的连接，并寻找下一个数据块的最佳DataNode进行读取，直到文件读取完成。

#### **3. 高可用原理**

早期HDFS的单NameNode是单点故障。HA通过**主备NameNode**机制解决。

* **Active NameNode** ：处理所有客户端请求。
* **Standby NameNode** ：热备节点，时刻与Active节点保持同步。
* **共享存储** ：两个NameNode通过一个共享的编辑日志存储（通常是QJM或NFS）来保持元数据同步。Active NN将编辑日志写入共享存储，Standby NN持续读取并应用这些日志，从而保持状态与Active NN几乎一致。
* **ZKFC** ：一个独立的故障转移控制器进程。它监控NameNode的健康状态，并通过ZooKeeper进行主备选举。如果Active NN故障，ZKFC会促使Standby NN切换为Active状态。
* **DataNode** ：会同时向两个NameNode发送心跳和块报告。

---

### **四、YARN：资源管理与任务调度基石**

YARN将Hadoop 1.0中的JobTracker的资源管理和作业调度/监控功能分离开来，使得Hadoop可以运行除MapReduce之外的计算框架（如Spark、Flink等）。

#### **1. 架构**

* **ResourceManager**

  * **角色** ：集群资源的最终仲裁者，全局只有一个。
  * **职责** ：

  1. **调度器** ：根据容量、队列等约束，将系统资源分配给各个正在运行的应用程序。它只负责调度，不负责监控或跟踪应用程序的状态。
  2. **应用程序管理器** ：负责接收作业提交，为应用程序申请第一个Container（用于启动ApplicationMaster），并在ApplicationMaster失败时重启它。
* **NodeManager**

  * **角色** ：每个节点上的代理，管理单个节点的资源。
  * **职责** ：

  1. 管理本节点的资源（CPU、内存、磁盘、网络）使用情况。
  2. 启动并监控容器。
  3. 向ResourceManager汇报本节点的资源使用情况和容器状态。
* **ApplicationMaster**

  * **角色** ：**每个应用程序**一个（例如，每个MapReduce作业或Spark作业都有一个自己的AM）。
  * **职责** ：

  1. 向ResourceManager**协商**资源。
  2. 与NodeManager **通信** ，以启动和停止任务。
  3. 监控所有任务的状态，并在任务失败时重新申请资源以重启任务。
* **Container**

  * **角色** ：YARN中的资源抽象，封装了某个节点上一定量的资源（如CPU核心数和内存）。所有应用程序都是在Container中运行的。

#### **2. 任务调度流程**

以MapReduce作业为例：

1. **作业提交** ：客户端程序将作业（包括JAR包、配置文件和分片信息）提交到ResourceManager。
2. **启动ApplicationMaster** ：ResourceManager的应用程序管理器找到一个可用的NodeManager，分配一个Container，并在其中启动该作业的ApplicationMaster。
3. **资源申请** ：ApplicationMaster向ResourceManager注册，然后根据计算逻辑（如Map任务数量）向ResourceManager的调度器申请所需的资源（Container）。
4. **启动任务** ：ResourceManager为ApplicationMaster分配了Container后，ApplicationMaster与对应的NodeManager通信，在已分配的Container中启动任务（如MapTask或ReduceTask）。
5. **任务执行与监控** ：任务在Container中执行，并向ApplicationMaster汇报进度和状态。
6. **完成与注销** ：当所有任务执行完毕，ApplicationMaster向ResourceManager注销，并自行关闭。然后ResourceManager通知客户端作业完成。

---

### **五、MapReduce：分布式计算基石**

MapReduce的核心思想是 **“分而治之”** ，将一个大任务拆分成多个小任务，在大量计算机上并行处理，最后将结果合并。

#### **1. 思想（分而治之）**

* **Map（映射）** ：将输入数据分割成独立的 **分片** ，然后由一个Map任务集群并行处理。每个Map任务处理一个分片，输出一系列的中间键值对。
* **Shuffle（洗牌）** ：系统对Map输出的中间结果进行排序、分组和分发，将相同键的数据发送给同一个Reduce任务。这是MapReduce的“心脏”。
* **Reduce（归约）** ：Reduce任务接收Shuffle过来的、属于自己键的数据，进行聚合计算，最终产生输出结果。

#### **2. Shuffle过程详解**

Shuffle横跨Map端和Reduce端，是MapReduce中最复杂、最核心的部分。

* **Map端的Shuffle** :

1. **分区** ：Map任务输出的每个键值对，会根据**分区器**决定它属于哪个Reduce任务。默认分区器是Hash Partitioner（`key.hashCode() % reduce任务数`）。
2. **写入环形缓冲区** ：输出不会直接写磁盘，而是先写入一个 **环形内存缓冲区** （默认100MB）。这样做是为了减少磁盘I/O。
3. **排序与溢出** ：当缓冲区内容达到阈值（默认80%）时，一个后台线程会开始将缓冲区的内容**溢出**到磁盘，形成一个 **溢出文件** 。在溢出之前，线程会对缓冲区内的数据 **按分区进行排序** ，在每个分区内部 **按Key进行排序** 。
4. **合并** ：如果Map输出很大，可能会有多次溢出，生成多个溢出文件。在Map任务结束前，所有这些溢出文件会被**合并**成一个大的、已分区且已排序的输出文件。

* **Reduce端的Shuffle** :

1. **复制** ：Reduce任务启动 **复制线程** ，通过HTTP协议从各个已完成Map任务的节点上**拉取**属于自己的那部分数据。
2. **合并** ：当来自不同Map任务的数据被拉取到Reduce节点时，如果数据量较小，会先保存在内存中；如果数据量超过阈值，则会像Map端一样 **合并溢出到磁盘** 。在合并过程中，会对数据进行 **归并排序** 。
3. **Reduce阶段** ：当所有Map任务的数据都被拉取过来并合并好后，就形成了一个整体有序的数据文件。Reduce任务开始执行用户自定义的 `reduce`函数，迭代处理这个已排序的、相同Key的数据分组，最终将结果写入HDFS。

 **总结** ：Shuffle的本质是 **“对Map输出的中间结果，按Key进行全局排序和分组，并分发给正确的Reducer”** 。这个过程保证了所有相同的Key最终都会汇聚到同一个Reduce任务中进行处理。

#### 3. 数据仓库与数据建模

* **经典数据仓库理论** ：维度建模（星型模型、雪花模型）、事实表与维度表、缓慢变化维（SCD）的处理方式。
* **大数据数仓分层** ：ODS -> DWD -> DWS -> ADS，理解每一层的职责和作用。
* **数据治理** ：数据质量、数据血缘、元数据管理（了解概念即可）。

#### 4. Linux与网络基础

* 常用命令：`grep`, `awk`, `sed`, `find`, `top`, `netstat`等。
* 网络知识：TCP/IP协议、HTTP协议。

---

### 六、 面试流程与准备策略

#### 1. 简历准备

* **项目经验是灵魂** ：准备1-2个你深度参与的大数据项目。
* **使用STAR法则** （情境、任务、行动、结果）来描述。
* **突出技术难点和你的解决方案** ：例如，如何解决数据倾斜？如何保证数据准确性？如何提升任务性能？
* **量化成果** ：例如，“通过优化，将任务运行时间从2小时缩短到30分钟”，“日处理数据量达TB级”。
* **技术栈匹配** ：根据JD要求，突出你精通的技术，略懂的可写“了解”。

#### 2. 面试环节

通常包括： **技术一面/二面 -> 主管/总监面 -> HR面** 。

* **技术面** ：
* **基础知识** ：围绕上述技术栈进行深度考察。
* **项目深挖** ：面试官会不断追问项目的细节，直到你答不上来，以此考察你的技术深度和真实性。
* **场景设计/编程题** ：
  * “设计一个实时数据大屏的架构。”
  * “如何统计一个网站的日活/月活？”
  * “手写SQL/Spark代码解决一个具体问题（如排名、Top N）。”
  * “手写算法题（LeetCode中等难度，如链表、树、动态规划）。”
* **主管/总监面** ：
* 考察技术广度、架构思维、业务理解能力、沟通协作和潜力。
* 问题可能包括：“你对大数据未来发展趋势怎么看？”、“你如何管理一个数据项目？”、“当你和业务方有分歧时怎么办？”
* **HR面** ：
* 考察软实力、职业规划、薪资期望、稳定性。
* 准备好回答：“你为什么离职？”、“你的职业规划是什么？”、“你的优缺点是什么？”

---

### 七、 高频面试真题举例

#### 1. SQL 篇

* 行列转换。
* 求连续登录N天的用户。
* 求每个班级的前N名。
* 求用户的累计消费金额。
* 如何去除重复数据？

#### 2. Spark 篇

* Spark Shuffle 和 MapReduce Shuffle 的区别？
* 详细解释一下 Spark 的宽依赖和窄依赖。
* 如何处理 Spark 数据倾斜？有哪些方案？
* RDD、DataFrame、DataSet 三者的区别和联系？
* Spark SQL 的优化手段有哪些？

#### 3. Kafka 篇

* Kafka 为什么速度快？
* 如何保证 Kafka 消息不丢失、不重复？
* Kafka 的 ISR 机制是什么？
* 消费者组（Consumer Group）的再平衡（Rebalance）是什么？

#### 4. 项目与场景设计篇

* 你项目中数据仓库是如何分层的？为什么这么分？
* 如果业务库的一张表数据量巨大（亿级别），如何高效同步？
* 设计一个实时ETL流程，将MySQL的Binlog数据同步到Hive/ClickHouse中。
* 如何监控和保障你负责的数据任务的稳定性？

---

### 八、 面试技巧与注意事项

1. **诚实，不要夸大** ：对于不熟悉的技术，坦诚说“了解”或“没在生产环境用过”，并表达出学习的意愿。被问穿比不会更糟糕。
2. **引导面试官** ：在回答问题时，可以主动延伸到相关知识点，展示你的知识体系。例如，被问到HDFS，可以提到它的高可用机制。
3. **沟通与表达** ：回答要有逻辑，分点陈述。遇到复杂问题，可以先和面试官确认理解是否正确，再一步步推导。
4. **准备好你的问题** ：在面试结尾，向面试官提问能体现你的思考。例如：

* “团队目前面临的最大技术挑战是什么？”
* “公司的大数据平台技术栈未来有什么规划？”
* “这个岗位的晋升路径是怎样的？”

**最后，保持自信和积极的心态。** 大数据领域知识浩如烟海，没有人能全部掌握。面试官更看重的是你的 **学习能力、解决问题的思路和对技术的热情** 。
